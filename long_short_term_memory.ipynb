{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "012be0aa",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory (LSTM) with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d8a161",
   "metadata": {},
   "source": [
    "This notebook will demonstrate how to create, optimize and make predictions using the Long Short-Term Memory (LSTM) network.\n",
    "\n",
    "Specifically, we will implement the Long Short-Term Memory unit seen below, that predicts sequential data to predict the value of two different companies.\n",
    "\n",
    "<img src='images/lstm_image.001.png' style='width: 720px'>\n",
    "\n",
    "The training data (below) consist of stock prices for two different companies, **Company A** and **Company B**. The goal is to use the data from the first **4** days to predict what the price will be on the **5th** day. If we look closely at the data, we'll see that the only differences in the prices occur on Day **1** and Day **5**. So the LSTM has to remember what happened on Day **1** in order to predict what will happen on Day **5**.\n",
    "\n",
    "\n",
    "<img src='images/company_a_data.png' style='width: 360px'> <img src='images/company_b_data.png' style='width: 360px'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f735447",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117106b3",
   "metadata": {},
   "source": [
    "## Import the modules that will do all the work\n",
    "\n",
    "The very first thing we need to do is load a bunch of Python modules. These modules give us extra functionality to create a Long Short-Term Memory (LSTM) neural network and optimize the neural network's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c89f3dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e9d5ee",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762d1b3e",
   "metadata": {},
   "source": [
    "## Build a Long Short-Term Memory unit by hand using PyTorch\n",
    "\n",
    "A Long Short-Term Memory (LSTM) unit is a type of neural network, and that means we need to create a new class. \n",
    "We'll create the following methods:\n",
    "- `__init__()` to initialize the Weights and Biases and keep track of a few other house keeping things.\n",
    "- `lstm_unit()` to do the LSTM math.\n",
    "- `forward()` to make a forward pass through the unrolled LSTM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9529eecc-e131-473b-9046-0a040735f3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        torch.manual_seed(seed=18)\n",
    "        \n",
    "        mean = torch.tensor(0.0)\n",
    "        std = torch.tensor(1.0)\n",
    "\n",
    "        self.wlr1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wlr2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.blr1 = nn.Parameter(torch.tensor(0.0), requires_grad=True)\n",
    "\n",
    "        self.wpr1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wpr2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.bpr1 = nn.Parameter(torch.tensor(0.0), requires_grad=True)\n",
    "\n",
    "        self.wp1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wp2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.bp1 = nn.Parameter(torch.tensor(0.0), requires_grad=True)\n",
    "\n",
    "        self.wo1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wo2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.bo1 = nn.Parameter(torch.tensor(0.0), requires_grad=True)\n",
    "\n",
    "    def lstm_unit(self, input_value, long_memory, short_memory):\n",
    "        long_remember_percent = torch.sigmoid((short_memory * self.wlr1) + \n",
    "                                             (input_value * self.wlr2) + \n",
    "                                             self.blr1)\n",
    "\n",
    "        potential_remember_percent = torch.sigmoid((short_memory * self.wpr1) +\n",
    "                                                   (input_value * self.wpr2) +\n",
    "                                                   self.bpr1)\n",
    "        \n",
    "        potential_memory = torch.tanh((short_memory * self.wp1) + \n",
    "                                      (input_value * self.wp2) +\n",
    "                                      self.bp1)\n",
    "\n",
    "        updated_long_memory = ((long_memory * long_remember_percent)+\n",
    "                               (potential_remember_percent * potential_memory))\n",
    "\n",
    "        output_percent = torch.sigmoid((short_memory * self.wo1) +\n",
    "                                       (input_value * self.wo2))\n",
    "\n",
    "        updated_short_memory = torch.tanh(updated_long_memory) * output_percent\n",
    "\n",
    "        return ([updated_long_memory, updated_short_memory])\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        long_memory = 0\n",
    "        short_memory = 0\n",
    "\n",
    "        day1 = X[0]\n",
    "        day2 = X[1]\n",
    "        day3 = X[2]\n",
    "        day4 = X[3]\n",
    "\n",
    "        long_memory, short_memory = self.lstm_unit(day1, long_memory, short_memory)\n",
    "        long_memory, short_memory = self.lstm_unit(day2, long_memory, short_memory)\n",
    "        long_memory, short_memory = self.lstm_unit(day3, long_memory, short_memory)\n",
    "        long_memory, short_memory = self.lstm_unit(day4, long_memory, short_memory)\n",
    "\n",
    "        return short_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8df0f7",
   "metadata": {},
   "source": [
    "Once we have created the class that defines an LST, we can use it to create a model and print out the randomly initialized Weights and Biases. Then, we'll see what those random Weights and Biases predict for **Company A** and **Company B**. If they are good predictions, then we're done! However, the chances of getting good predictions from random values is very small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28941509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paramaters before optimization\n",
      "wlr1 : 0.5940740704536438\n",
      "wlr2 : -0.12711703777313232\n",
      "blr1 : 0.0\n",
      "wpr1 : -0.7286937236785889\n",
      "wpr2 : 0.7211949229240417\n",
      "bpr1 : 0.0\n",
      "wp1 : -0.566031277179718\n",
      "wp2 : 0.5780901908874512\n",
      "bp1 : 0.0\n",
      "wo1 : 0.30693256855010986\n",
      "wo2 : 0.6139482259750366\n",
      "bo1 : 0.0\n",
      "Company A: Observed = 0, Predicted = tensor(0.2416)\n",
      "Company B: Observed = 1, Predicted = tensor(0.2482)\n"
     ]
    }
   ],
   "source": [
    "model = LSTM()\n",
    "model.eval()\n",
    "print('Paramaters before optimization')\n",
    "for name, parameter in model.named_parameters():\n",
    "    print(f'{name} : {parameter.data}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    print('Company A: Observed = 0, Predicted =', model(torch.tensor([0., 0.5, 0.25, 1.])))\n",
    "    print('Company B: Observed = 1, Predicted =', model(torch.tensor([1., 0.5, 0.25, 1.])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14cd909",
   "metadata": {},
   "source": [
    "With the unoptimized parameters, the predicted value for **Company A**, **0.2416**, is quite far from the observed value, **0**. On the other hand, the predicted value for **Company B**, **0.2482**, is terrible, because it is relatively far from the observed value, **1**. So, that means we need to train the LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfad86cd",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb9b8d6",
   "metadata": {},
   "source": [
    "## Train the LSTM unit and use TensorBoard to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "900f2801",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[0., 0.5, 0.25, 1.], [1., 0.5, 0.25, 1.]])\n",
    "labels = torch.tensor([0., 1.])\n",
    "\n",
    "dataset = TensorDataset(inputs, labels)\n",
    "dataloader = DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdc10bf",
   "metadata": {},
   "source": [
    "**NOTE:** We are starting with **20** epochs. This may be enough to successfully optimize all of the parameters, but it might not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6000b083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 / 20 | Loss: 0.7896659038960934\n",
      "Epoch: 2 / 20 | Loss: 0.6475228294730186\n",
      "Epoch: 3 / 20 | Loss: 0.5497229248285294\n",
      "Epoch: 4 / 20 | Loss: 0.4833335056900978\n",
      "Epoch: 5 / 20 | Loss: 0.4527108073234558\n",
      "Epoch: 6 / 20 | Loss: 0.4500923603773117\n",
      "Epoch: 7 / 20 | Loss: 0.4613257348537445\n",
      "Epoch: 8 / 20 | Loss: 0.4745378643274307\n",
      "Epoch: 9 / 20 | Loss: 0.4838353246450424\n",
      "Epoch: 10 / 20 | Loss: 0.487785741686821\n",
      "Epoch: 11 / 20 | Loss: 0.48706820607185364\n",
      "Epoch: 12 / 20 | Loss: 0.4830686151981354\n",
      "Epoch: 13 / 20 | Loss: 0.477227047085762\n",
      "Epoch: 14 / 20 | Loss: 0.4707200676202774\n",
      "Epoch: 15 / 20 | Loss: 0.4642821252346039\n",
      "Epoch: 16 / 20 | Loss: 0.45812784135341644\n",
      "Epoch: 17 / 20 | Loss: 0.4519989490509033\n",
      "Epoch: 18 / 20 | Loss: 0.445335254073143\n",
      "Epoch: 19 / 20 | Loss: 0.43752019107341766\n",
      "Epoch: 20 / 20 | Loss: 0.4281167984008789\n",
      "Company A: Observed = 0, Predicted = tensor(0.4432)\n",
      "Company B: Observed = 1, Predicted = tensor(0.5343)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "model_path = os.path.join(os.path.abspath(os.getcwd()), 'models/lstm_last.pt')\n",
    "epochs = 20\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    outputs = [0, 0]\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        output = model(X[0])\n",
    "        loss = (y - output) ** 2\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        outputs[batch] = output\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    writer.add_scalar('Loss/Train', total_loss, epoch + 1)\n",
    "    writer.add_scalar('Out0/Train', outputs[0], epoch + 1)\n",
    "    writer.add_scalar('Out1/Train', outputs[1], epoch + 1)\n",
    "\n",
    "    print(f'Epoch: {epoch + 1} / {epochs} | Loss: {total_loss}')\n",
    "    torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': total_loss,\n",
    "            'outputs': outputs\n",
    "            }, model_path)\n",
    "    # print(f'Model saved at {model_path}')\n",
    "\n",
    "writer.flush()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print('Company A: Observed = 0, Predicted =', model(torch.tensor([0., 0.5, 0.25, 1.])))\n",
    "    print('Company B: Observed = 1, Predicted =', model(torch.tensor([1., 0.5, 0.25, 1.])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d18702",
   "metadata": {},
   "source": [
    "Unfortunately, these predictions are terrible. So, it seems like we'll have to do more training. However, it would be great if we could be confident that more training will actually improve the predictions. If not, we can spare ourselves a lot of time, and potentially money, and just give up. So, before we dive into more training, let's look at the loss values and predictions that we saved in logfiles with **TensorBoard**. **TensorBoard** will graph everything that we logged during training, making it easier to see if things are headed in the right direction or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a086979c",
   "metadata": {},
   "source": [
    "Below are the graphs of **loss** (`train_loss`), the predictions for **Company A** (`out_0`), and the predictions for **Company B** (`out_1`). Remember for **Company A**, we want to predict **0** and for **Company B**, we want to predict **1**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57a2d3a",
   "metadata": {},
   "source": [
    "<img src='images/trainloss_20_epochs.png' style='width: 360px'> <img src='images/out_0_20_epochs.png' style='width: 360px'> <img src='images/out_1_20_epochs.png' style='width: 360px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84619828",
   "metadata": {},
   "source": [
    "If we look at the **loss** (`Loss/Train`), we see that it went down, which is good, but it started rising after a few epochs. When we look at the predictions for **Company A** (`Out0/Train`), we see that they started out pretty good, close to **0**, but then got really bad early on in training, shooting all the way up to **0.57**, but are starting to get smaller. In contrast, when we look at the predictions for **Company B** (`Out1/Train`), we see that they started out really bad, close to **0**, but have been getting better ever since and look like they could continue to get better if we kept training.\n",
    "\n",
    "In summary, the graphs seem to suggest that if we continued training our model, the predictions would improve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9040fd9",
   "metadata": {},
   "source": [
    "## Adding More Epochs without Starting Over\n",
    "\n",
    "Since we are saving a general checkpoint of the model during training, we can pick up where we left off training without having to start from scratch. This is because when we save the model using `torch.save()`, we are saving a file that keeps track of the Weights and Biases, and any additional information we want as they change. As a result, all we have to do to pick up where we left off is to tell `torch.load` where the checkpoint file is located. Since we want to add **20** more epochs to the training,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4294c4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 / 40 | Loss: 0.41700156033039093\n",
      "Epoch: 22 / 40 | Loss: 0.40432092547416687\n",
      "Epoch: 23 / 40 | Loss: 0.39026327431201935\n",
      "Epoch: 24 / 40 | Loss: 0.37473373115062714\n",
      "Epoch: 25 / 40 | Loss: 0.35710014402866364\n",
      "Epoch: 26 / 40 | Loss: 0.33615151047706604\n",
      "Epoch: 27 / 40 | Loss: 0.3103083521127701\n",
      "Epoch: 28 / 40 | Loss: 0.27800555527210236\n",
      "Epoch: 29 / 40 | Loss: 0.23809392750263214\n",
      "Epoch: 30 / 40 | Loss: 0.19031646102666855\n",
      "Epoch: 31 / 40 | Loss: 0.13708627596497536\n",
      "Epoch: 32 / 40 | Loss: 0.08659545425325632\n",
      "Epoch: 33 / 40 | Loss: 0.04928428865969181\n",
      "Epoch: 34 / 40 | Loss: 0.028094633697037352\n",
      "Epoch: 35 / 40 | Loss: 0.017632327799219638\n",
      "Epoch: 36 / 40 | Loss: 0.011430630402173847\n",
      "Epoch: 37 / 40 | Loss: 0.007879333381424658\n",
      "Epoch: 38 / 40 | Loss: 0.006646450608968735\n",
      "Epoch: 39 / 40 | Loss: 0.004935314325848594\n",
      "Epoch: 40 / 40 | Loss: 0.004065959437866695\n",
      "Company A: Observed = 0, Predicted = tensor(-0.0188)\n",
      "Company B: Observed = 1, Predicted = tensor(0.9403)\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter()\n",
    "\n",
    "path_to_checkpoint = model_path\n",
    "\n",
    "epochs = 40\n",
    "\n",
    "model = LSTM()\n",
    "optimizer = Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "checkpoint = torch.load(model_path)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "outputs = checkpoint['outputs']\n",
    "\n",
    "writer.add_scalar('Loss/Train', loss, epoch)\n",
    "writer.add_scalar('Out0/Train', outputs[0], epoch)\n",
    "writer.add_scalar('Out1/Train', outputs[1], epoch)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epoch, epochs):\n",
    "    total_loss = 0\n",
    "    outputs = [0, 0]\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        output = model(X[0])\n",
    "        loss = (y - output) ** 2\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        outputs[batch] = output\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    writer.add_scalar('Loss/Train', total_loss, epoch + 1)\n",
    "    writer.add_scalar('Out0/Train', outputs[0], epoch + 1)\n",
    "    writer.add_scalar('Out1/Train', outputs[1], epoch + 1)\n",
    "\n",
    "    print(f'Epoch: {epoch + 1} / {epochs} | Loss: {total_loss}')\n",
    "\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': total_loss,\n",
    "        'outputs': outputs\n",
    "    }, model_path)\n",
    "\n",
    "writer.flush()\n",
    "        \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print('Company A: Observed = 0, Predicted =', model(torch.tensor([0., 0.5, 0.25, 1.])))\n",
    "    print('Company B: Observed = 1, Predicted =', model(torch.tensor([1., 0.5, 0.25, 1.])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e35c86",
   "metadata": {},
   "source": [
    "<img src='images/trainloss_40_epochs.png' style='width: 360px'> <img src='images/out_0_40_epochs.png' style='width: 360px'> <img src='images/out_1_40_epochs.png' style='width: 360px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c663877d",
   "metadata": {},
   "source": [
    "The graphs are much better than before. The blue lines in each graph represents the values we logged during the extra **20** epochs. The **loss** is getting smaller and the predictions for both companies are improving. However, because it looks like there is even more room for improvement, let's add **60** more epochs to the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc11018f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41 / 100 | Loss: 0.0036251981218811125\n",
      "Epoch: 42 / 100 | Loss: 0.0028253277869225712\n",
      "Epoch: 43 / 100 | Loss: 0.002830698009347543\n",
      "Epoch: 44 / 100 | Loss: 0.0022479911965547217\n",
      "Epoch: 45 / 100 | Loss: 0.002265945033286698\n",
      "Epoch: 46 / 100 | Loss: 0.0018887358705796942\n",
      "Epoch: 47 / 100 | Loss: 0.0019249996839789674\n",
      "Epoch: 48 / 100 | Loss: 0.0016605532772473452\n",
      "Epoch: 49 / 100 | Loss: 0.001677868094702717\n",
      "Epoch: 50 / 100 | Loss: 0.001494286176111359\n",
      "Epoch: 51 / 100 | Loss: 0.001512537884991616\n",
      "Epoch: 52 / 100 | Loss: 0.001385268818012264\n",
      "Epoch: 53 / 100 | Loss: 0.0013735894935962278\n",
      "Epoch: 54 / 100 | Loss: 0.0013034240091656102\n",
      "Epoch: 55 / 100 | Loss: 0.0012723032541543944\n",
      "Epoch: 56 / 100 | Loss: 0.0012392868302413262\n",
      "Epoch: 57 / 100 | Loss: 0.0011907595173852314\n",
      "Epoch: 58 / 100 | Loss: 0.0011813984219770646\n",
      "Epoch: 59 / 100 | Loss: 0.0011354731411188368\n",
      "Epoch: 60 / 100 | Loss: 0.0011213664838578552\n",
      "Epoch: 61 / 100 | Loss: 0.0010925589126600244\n",
      "Epoch: 62 / 100 | Loss: 0.0010705772831443028\n",
      "Epoch: 63 / 100 | Loss: 0.0010534873249525845\n",
      "Epoch: 64 / 100 | Loss: 0.0010276934678650207\n",
      "Epoch: 65 / 100 | Loss: 0.001014817815303104\n",
      "Epoch: 66 / 100 | Loss: 0.0009942545382273238\n",
      "Epoch: 67 / 100 | Loss: 0.0009762514309841208\n",
      "Epoch: 68 / 100 | Loss: 0.0009629084829612111\n",
      "Epoch: 69 / 100 | Loss: 0.0009446054161017514\n",
      "Epoch: 70 / 100 | Loss: 0.000930244788435175\n",
      "Epoch: 71 / 100 | Loss: 0.0009163175433286597\n",
      "Epoch: 72 / 100 | Loss: 0.0009010068862451703\n",
      "Epoch: 73 / 100 | Loss: 0.0008879425288341736\n",
      "Epoch: 74 / 100 | Loss: 0.0008745891161083819\n",
      "Epoch: 75 / 100 | Loss: 0.0008612949144080062\n",
      "Epoch: 76 / 100 | Loss: 0.0008490093524073927\n",
      "Epoch: 77 / 100 | Loss: 0.0008366424451367038\n",
      "Epoch: 78 / 100 | Loss: 0.0008246058547030088\n",
      "Epoch: 79 / 100 | Loss: 0.0008130620476407557\n",
      "Epoch: 80 / 100 | Loss: 0.0008016650521511792\n",
      "Epoch: 81 / 100 | Loss: 0.0007905213485202012\n",
      "Epoch: 82 / 100 | Loss: 0.0007797170666634656\n",
      "Epoch: 83 / 100 | Loss: 0.0007691787831731745\n",
      "Epoch: 84 / 100 | Loss: 0.0007587666441946084\n",
      "Epoch: 85 / 100 | Loss: 0.0007486545745010176\n",
      "Epoch: 86 / 100 | Loss: 0.0007388516947628432\n",
      "Epoch: 87 / 100 | Loss: 0.0007291133393572125\n",
      "Epoch: 88 / 100 | Loss: 0.0007196268751257229\n",
      "Epoch: 89 / 100 | Loss: 0.0007104625147533739\n",
      "Epoch: 90 / 100 | Loss: 0.0007013335214225286\n",
      "Epoch: 91 / 100 | Loss: 0.0006924418852729541\n",
      "Epoch: 92 / 100 | Loss: 0.0006838246543274806\n",
      "Epoch: 93 / 100 | Loss: 0.0006752712895145008\n",
      "Epoch: 94 / 100 | Loss: 0.000666938694513064\n",
      "Epoch: 95 / 100 | Loss: 0.0006587981480699057\n",
      "Epoch: 96 / 100 | Loss: 0.0006507671279319116\n",
      "Epoch: 97 / 100 | Loss: 0.0006429529800957567\n",
      "Epoch: 98 / 100 | Loss: 0.0006352527809485586\n",
      "Epoch: 99 / 100 | Loss: 0.0006277095986217951\n",
      "Epoch: 100 / 100 | Loss: 0.0006203422308175277\n",
      "Company A: Observed = 0, Predicted = tensor(2.4159e-05)\n",
      "Company B: Observed = 1, Predicted = tensor(0.9752)\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter()\n",
    "\n",
    "path_to_checkpoint = model_path\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "model = LSTM()\n",
    "optimizer = Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "checkpoint = torch.load(model_path)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "outputs = checkpoint['outputs']\n",
    "\n",
    "writer.add_scalar('Loss/Train', loss, epoch)\n",
    "writer.add_scalar('Out0/Train', outputs[0], epoch)\n",
    "writer.add_scalar('Out1/Train', outputs[1], epoch)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epoch, epochs):\n",
    "    total_loss = 0\n",
    "    outputs = [0, 0]\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        output = model(X[0])\n",
    "        loss = (y - output) ** 2\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        outputs[batch] = output\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    writer.add_scalar('Loss/Train', total_loss, epoch + 1)\n",
    "    writer.add_scalar('Out0/Train', outputs[0], epoch + 1)\n",
    "    writer.add_scalar('Out1/Train', outputs[1], epoch + 1)\n",
    "\n",
    "    print(f'Epoch: {epoch + 1} / {epochs} | Loss: {total_loss}')\n",
    "\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': total_loss,\n",
    "        'outputs': outputs\n",
    "    }, model_path)\n",
    "\n",
    "writer.flush()\n",
    "        \n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    print('Company A: Observed = 0, Predicted =', model(torch.tensor([0., 0.5, 0.25, 1.]).detach()))\n",
    "    print('Company B: Observed = 1, Predicted =', model(torch.tensor([1., 0.5, 0.25, 1.]).detach()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd533b78",
   "metadata": {},
   "source": [
    "<img src='images/trainloss_100_epochs.png' style='width:360px'> <img src='images/out_0_100_epochs.png' style='width:360px'> <img src='images/out_1_100_epochs.png' style='width:360px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ded818",
   "metadata": {},
   "source": [
    "After **100** epochs, the graphs have converged. The prediction for **Company A** is super close to **0**, which is exactly what we want, and the prediction for **Company B** is close to **1**, which is also what we want.\n",
    "\n",
    "\n",
    "The red lines show how things changed when we added an additional **60** epochs to the training, for a total of **100** epochs. Now we see that the **loss** (`Loss/Train`) and the predictions for each company appear to be tapering off, suggesting that adding more epochs may not improve the predictions much, so we're done with the training.\n",
    "\n",
    "\n",
    "Finally, let's print out the final estimates for the Weights and Biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91b4d0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paramaters after optimization\n",
      "wlr1 : 2.893209457397461\n",
      "wlr2 : 1.0485767126083374\n",
      "blr1 : 1.544776439666748\n",
      "wpr1 : 1.0238780975341797\n",
      "wpr2 : 1.5359022617340088\n",
      "bpr1 : 0.6787620186805725\n",
      "wp1 : 2.1890065670013428\n",
      "wp2 : 1.3072530031204224\n",
      "bp1 : -0.39732617139816284\n",
      "wo1 : 2.5714805126190186\n",
      "wo2 : 1.557863712310791\n",
      "bo1 : 0.0\n"
     ]
    }
   ],
   "source": [
    "print('Paramaters after optimization')\n",
    "for name, parameter in model.named_parameters():\n",
    "    print(f'{name} : {parameter.data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6149470a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
